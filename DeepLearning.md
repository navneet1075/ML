DeepLearning topics and bullet points


1. hyperparameters in the multi layer perceptron 
  a) number of hidden layers
  b) number of neurons per layer
  c) activation function
  d) learning rate
  e) batch size
  f) number of epochs
  
  
  Concepts and main points for reference :
  
  1. early stopping in neural networks training with params (like patience) .patience specifies how many iterations before stopping the training even if the loss does not reduce.
  2.  loss functions 
  3. activation functions : relu, tanh, sigmoid, leaky relu, softmax
  4. weight calculation : w(l) = zx(l-1)+b(l) where l is the last layer 
  5. backpropagation
  6. chain rule for calculating derivatives.
  7. benefits of different activation functions.
  8. large neural networks almost never get stuck in local minima, and even when they do these local optima are almost as good as the global optimum. However, they can still get stuck on long plateaus for a long time.
 
